### Explain following concepts, and how they coordinate with each other:
**Topic** - A topic in Kafka is a category or feed name where records are stored.
It is similar to a table in a database or a folder in a filesystem.
Topics allow Kafka to organize streams of data into groups. Each topic can have multiple partitions to parallelize message handling.  
**Partition** - A partition is a subset of a topic, which allows Kafka to distribute the load and improve scalability. 
Each partition is an ordered sequence of records that is immutable.
Each partition is replicated across multiple brokers for fault tolerance.  
**Broker** - A broker is a Kafka server that stores data and serves clients (producers and consumers).
Kafka brokers handle partition leadership and ensure data replication among other brokers for fault tolerance.  
**Consumer group** - A consumer group is a group of consumers that work together to consume messages from a Kafka topic.
Kafka ensures that each partition is read by only one consumer within a consumer group. This allows the load to be 
distributed across multiple consumers while preventing duplicate message processing.  
**Producer** - A producer is a client that publishes (writes) messages to Kafka topics. It decides which topic and 
partition to send the messages to, either using a round-robin approach or based on a specific key.  
**Offset** - An offset is a unique identifier assigned to each message in a partition. Consumers use offsets to keep 
track of which messages have been read.
The offset is critical for ensuring consumers do not reprocess the same message.  
**Zookeeper** - Zookeeper is used by Kafka for managing metadata about the Kafka cluster, including brokers and partition 
leadership. It ensures synchronization and maintains configuration data.
Starting with Kafka 2.8, Kafka can run without Zookeeper, relying on an internal Kafka Raft (KRaft) protocol for metadata management.  



### 1. Given N (number of partitions) and M (number of consumers,) what will happen when N>=M and N<M respectively?
When N>=M, each consumer consumes data from at least one partition, some consumers may consume from more than one partition. 
This setup ensures all partitions are consumed, and the consumers' load is distributed. When N<M, some consumers may not 
receive any data. 
### 2. Explain how brokers work with topics?
Brokers manage and store data for topics. A Kafka topic is divided into one or more partitions which are distributed across 
multiple brokers in the Kafka cluster. Each broker is responsible for storing one or more partitions of a topic, including
the data (messages) written to those partitions.
Every partition is replicated across multiple brokers for fault tolerance, with one broker serving as the leader for 
each partition and others as followers.
### 3. Are messages pushed to consumers or consumers pull messages from topics?
Consumers pull messages from topics. 
### 4. How to avoid duplicate consumption of messages?
1. commit offsets after processing
2. use idempotent processing - designing the consumer application so that if a message is processed more than once, the
outcome remains the same. 
3. Enable Kafka's Exactly-Once Semantics(EOS) - ensure that messages are neither lost nor duplicated during processing
4. Use Kafka Connect or Kafka Streams for Exactly-Once Processing
### 5. What will happen if some consumers are down in a consumer group? Will data loss occur? Why?
Data loss will not occur.  
1. Partition rebalancing. When a consumer in a consumer group goes down, Kafka will automatically trigger a rebalance. Kafka
redistributes the partitions that were assigned to the downed consumer to the remaining active consumers. 
2. Kafka guarantees that data is not lost when a consumer goes down since messages are stored in brokers. 
3. The new consumer takes over the partition and starts reading from the last committed offset. This might lead to duplicate
processing but no data loss. This is part of Kafka's default at-least-once delivery guarantee. 
4. The overall throughput of the system may be reduced during rebalancing while the remaining consumers adjust to the new workload. 
### 6. What will happen if an entire consumer group is down? Will data loss occur? Why?
Data loss will not occur.  
1. Kafka keep messages on a retention policy and will not delete them within this window. 
2. Message consumption stops when the entire consumer group goes down. 
### 7. Explain consumer lag and how to resolve it?
Consumer lag refers to the difference between the latest message offset and the last committed offset. Producer produces 
faster than consumer so the consumers are left behind.  
- Latest offset: the offset of the most recent message produced to a Kafka partition
- committed offset: the offset of the last message that the consumer has successfully processed and acknowledged. 
- Consumer lag=latest offset-commited offset
To solve consumer lag 
1. scale out consumers 
2. optimize message processing (increase throughput)
3. increase consumer parallelism
4. optimize consumer configuration settings 
   1. increase batch size
5. monitor and adjust kafka broker settings 
   1. increase broker throughput - ensure that the brokers have enough CPU, memory, and network bandwidth to handle both producer writes and consumer writes
   2. replicas and ack settings - ensure that the Kafka brokers are not overloaded by producers by reducing producer ack requirements
6. handle consumer rebalancing efficiently
### 8. Explain how Kafka tracks message delivery?
1. message offset tracking
Kafka partition is an ordered, immutable sequence of records, with a unique offset.
2. consumer group coordination
3. delivery semantics
    - At-least-once delivery
4. ack mechanisms
    - producer waits for ack from broker. 
### 9. Compare Kafka vs RabbitMQ, compare messageing frameworks vs MySql (Why Kafka)?
#### Kafka vs RabbitMQ
1. Kafka is log based storage, RabitMQ is MQ 
2. Kafka has partitions, queues are spread across nodes w/o partitions. 
3. Kafka has data retention
4. Kafka broker is distributed
5. Kafka has higher throughput, higher latency
#### Kafka/RabbitMQ vs Mysql
1. Kafka is purpose-built for handling large-scale, real-time event streams with low overhead and can be scaled horizontally. 
2. Kafka allows decoupling producer and consumers
3. Kafka has ability to retain message and replay messages, which is crucial for event-driven architectures, data pipelines, 
and stream processing. 
4. Kafka has lower latency and high throughput. 